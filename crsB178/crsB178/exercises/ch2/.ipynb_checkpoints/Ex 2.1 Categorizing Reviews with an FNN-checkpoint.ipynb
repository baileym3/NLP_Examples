{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70bb7978-9ca0-4c2a-a083-dff1333d7bc1",
   "metadata": {},
   "source": [
    "# Ex1.1 Categorizing Reviews with an FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481224a-f60e-4219-9f4d-fb3798e5b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#Ignore the next statement -- it is just to estimate how long the exercise takes\n",
    "start = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373aff2-dc8f-4ec7-92aa-e98249f0b672",
   "metadata": {},
   "source": [
    "We shall use a neural network to categorize user reviews of articles in Wikipedia. The aim is to identify the reviews which contain personal attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d9197-a494-4002-b36e-9f208b78f8fc",
   "metadata": {},
   "source": [
    "The dataset we will use includes over 100k labeled discussion comments from English Wikipedia. Each comment was labeled by multiple annotators via Crowdflower on whether it contains a personal attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc559a31-3047-4711-be7d-c2d042e7dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import urllib\n",
    "import sklearn\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39d6c33-4520-4b40-bc1f-bc35179ab17f",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "There are two files, one with the comments and another with annotations made by reviewers as to whether the comments contain personal attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd368ab-5a2e-41f8-9ae5-4e7ec814e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8542f535-b763-4bb6-8b00-32ccc0fb864c",
   "metadata": {},
   "source": [
    "## Examining the data\n",
    "First we look at the comments dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd29eb-cc0e-47db-94f5-0fcdfe54d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52fc66-ca9a-4d1f-b71b-8a62baaa22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b6b191-b695-400d-b555-8ea6b9b0917a",
   "metadata": {},
   "source": [
    "The first column is the review ID. Each user review of an article has a rev_id. The other column of interest is the comment column. It will need a bit of cleaning. The other columns are irrelevant to our purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed94ae-f155-410d-b7d4-07d7e37221e9",
   "metadata": {},
   "source": [
    "We now look at the annotations dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e9c9f-4bff-4d47-a757-0105f4f9fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baffa08-96ef-4f4a-9c3c-4dc9361d45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2ffad2-65e9-4f56-bf72-eda9cef1d50a",
   "metadata": {},
   "source": [
    "Each comment was given to multiple \"workers\" and the workers scored it for various types of attacks.  The results are in the second dataset. The rev_id is the link between the two datasets. A `1` means that worker considered the comment to be an attack. The last column, \"attack\" will be a `1` if any of the other columns for specific types of attack are `1`'s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9a08d-6294-4838-8a37-016f4bc78f29",
   "metadata": {},
   "source": [
    "Let's find some records where the attack column has a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd61088-22e2-46f5-b8ab-3a8bf058f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[annotations[\"attack\"]==1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370b301-498f-42cb-b5cb-3ce0c029ccb5",
   "metadata": {},
   "source": [
    " Consider, for example, a specific comment from the review ith rev_id = 89320.\n",
    " \n",
    " Several workers, the ones with worker_id 3341, 3338, 2101 and 673 thought it had some kind of personal attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d65829-72a9-404b-97a7-58eca10927f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.loc[89320][\"comment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39a4a5-2488-40ac-b323-f86fcb4343bc",
   "metadata": {},
   "source": [
    "We can look at the results from all the workers who scored this comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d863f-5c3e-4934-aaee-d074820e3838",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[annotations[\"rev_id\"]==89320]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71fe1b7-204f-4609-af41-08ec255ec641",
   "metadata": {},
   "source": [
    "We shall consider a comment to be an attack if its mean score in the attack column is above 0.5. We create a column called label\".\n",
    "We group the annotations by rev_id and if the mean for the attack colummn is above 0.5 the label is true, otherwise it is false. We add that column to the comments dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d06b68-aa51-4f72-bf00-eb21a91213e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b67032d-c0f9-46e2-a299-98a930547c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0125ac0-0277-4328-9d12-f6c8ffa27659",
   "metadata": {},
   "source": [
    "We join comments and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b2b94-5299-4a79-83bc-e3ab284a2d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.join(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70bc34-7c59-4377-9e89-36a33541becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf455f30-c141-43b8-aa97-9a0a6ca01475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this cell if you would rather not read rather unpleasant comments!\n",
    "comments[comments[\"attack\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5af81-7cd2-468c-8305-911620681b2c",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a2756-4603-493e-a96a-7746cb0714bd",
   "metadata": {},
   "source": [
    "We remove the \"NEWLINE_TOKEN\" and \"TAB_TOKEN\" substrings in the comments as well as the punctuation. We shall lower case the words and remove stop words and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d79f354-5d44-4bc6-b1cd-0dde8e762c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['comment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec99a2-fa22-42cd-bc64-20106462ab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \"\"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \"\"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.lower())\n",
    "comments['comment'] = comments['comment'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: re.sub(r'\\d+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf597f7f-3e85-4977-af45-ce2b50a4057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['comment'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a61886-e988-4d57-ae58-51c4798aabbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ced8b-ed14-4589-83b3-60cc63d9e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(comment):\n",
    "    word_tokens = word_tokenize(comment)\n",
    "    filtered_comment = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_comment = \"\"\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_comment = filtered_comment + \" \" + w\n",
    "    return(filtered_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6316e-101b-4d15-95de-f10929160ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stop_words(\"This is a test comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2608db-b032-4896-9d27-1c1ad91f147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This takes about 30 seconds\n",
    "comments['comment'] = comments['comment'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641d921-a455-43d1-912e-76f0a2da75c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['comment'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc18553-0817-4edf-83ae-071d76ed0229",
   "metadata": {},
   "source": [
    "We only need the comment and attack columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b816a-0e89-461c-ac89-a6794057ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([comments[\"comment\"],comments[\"attack\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732aa0e-4e01-4c45-becb-1f3bc3b6f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b06b1c-8fbf-4674-a968-83e1427be63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9989a16-f950-40ca-b01f-b70a16ee64a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"attack\"] == True].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48121405-e707-47ae-8cc3-a36701281c49",
   "metadata": {},
   "source": [
    "## Splitting the data into a training set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfbaabc-9978-4e2a-9f11-2d8a65f9980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['comment'], \n",
    "    df['attack'], \n",
    "    test_size = 0.2,\n",
    "    random_state = 1278)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23948bf0-2d35-4f01-8b02-9c17992acd34",
   "metadata": {},
   "source": [
    "As a sanity check, print out the shapes of the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affc088-11e9-415f-a219-f080e65621f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training features and labels\")\n",
    "print(\"X_train shape: \",X_train.shape)\n",
    "print(\"y_train shape: \",y_train.shape)\n",
    "print()\n",
    "print(\"Testing features and labels\")      \n",
    "print(\"X_test shape: \",X_test.shape)\n",
    "print(\"y_test shape: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf0fd73-4836-4c49-9c7c-585cf91aa713",
   "metadata": {},
   "source": [
    "Vectorize the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0101a220-1a24-4658-b09b-91679f5b4f13",
   "metadata": {},
   "source": [
    "The features need to be expressed as vectors. We shall use CountVectorizer which does a word count on each document and creates a vector fo it based on the frequency of words in it. To avoid very  long vectors we shall just use the 5000 most frequent words as features. This is after removing stop words as they are very frequent but carry no information about the document. We also will not use rare words, words that appear in less than 10% of the documents. Further, we will not use words that are too common, ones that are present in more than 90% of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a01a51-74fc-4031-8580-64b920d1186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c3019f-1520-4be8-a899-8660b2bfed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary = True, \n",
    "                             stop_words = stopwords.words('english'), \n",
    "                             lowercase = True, \n",
    "                             min_df = 3, \n",
    "                             max_df = 0.9, \n",
    "                             max_features = 5000)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339ca20-b381-4dc1-a2e8-c4f31fe9b25b",
   "metadata": {},
   "source": [
    "The array produced by CountVectorizer is a sparse array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8862b023-7347-4956-97bb-933363de5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train_vectorized.toarray().shape)\n",
    "print(X_train_vectorized.toarray()[5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dead281-d57d-464b-ad9c-2ddcb6803bbf",
   "metadata": {},
   "source": [
    "Each one of the 5000 words being used to characterize the comments has an index. If the word is present in the document the value at that index will be a 1, otherwise it will be a 0. There are relatively few 1's so a sparse matrix is an efficient way to store the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c0475-cdf5-4aae-9383-304a51e51bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the first 20 mappings of the form word => index.\n",
    "print(list(vectorizer.vocabulary_.items())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f440adb-9241-4a60-8e5f-03ac2e9f11a1",
   "metadata": {},
   "source": [
    "Some of the words used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd52a45-7302-4d23-92b1-26832b7b063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()[1200:1250]\n",
    "# Be warned as you explore that some words from media will be unpleasant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae54106-2949-4dbd-b422-4e9f05e2666c",
   "metadata": {},
   "source": [
    "## Defining the model (neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ac977-2ae5-4bfd-ba08-a5e8df79fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Sequential is a container for the other components.\n",
    "# You add layers, in order, to an instance of Sequential.\n",
    "\n",
    "nn = Sequential()\n",
    "\n",
    "# The 5000 features plus a bias, which makes 5001 items, will be fed to a dense layer with 500 nodes.\n",
    "# This layer calculates 5001 * 500 = 2,500,500 weights.\n",
    "\n",
    "nn.add(Dense(units = 500, activation = 'relu', input_dim = len(vectorizer.get_feature_names())))\n",
    "\n",
    "# You get an output from each node, 500 outputs in all.\n",
    "# The 500 outputs of the first hidden layer plus a bias will go to the one node of the \n",
    "# second hidden layer. This makes 501 weights to calculate in this layer.\n",
    "\n",
    "nn.add(Dense(units=1, activation='sigmoid'))\n",
    "  \n",
    "# Binary cross entropy is a popular loss function for binary type (yes/no) situations.\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed69a76-88fd-4e73-98c3-a0cf21d71216",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002c8519-a7f0-4b86-b96e-96e4e110b6bb",
   "metadata": {},
   "source": [
    "This very small network has 2.5 million parameters to calculate.\n",
    "Make sure there are no other kernels running otherwise your kernel is likely to crash for lack of resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde84713-681e-48a3-a9bd-569d93e7f72d",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80499102-4343-47b3-a463-0a19d17caac1",
   "metadata": {},
   "source": [
    "This will take about 7 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf730ac8-3066-4bcf-8c15-e01fb341797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Takes 3 to 7 min depending on resources available.\n",
    "# The last 2000 rows of the training data are used for validation.\n",
    "nn.fit(X_train_vectorized[:-2000].toarray(), y_train[:-2000], \n",
    "          epochs = 4, batch_size = 128, verbose = 1, \n",
    "          validation_data = (X_train_vectorized[-2000:].toarray(), y_train[-2000:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bce662-cb80-4179-a5d1-164c8ade1c92",
   "metadata": {},
   "source": [
    "## Evaluating the model peformance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c4f173-f257-4a93-bfd0-eacc3e619c9b",
   "metadata": {},
   "source": [
    "We prepare vectors for the test data set and use the `evaluate()` method so see how good the model is with unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b643e18-c4e8-4ec9-8132-c5ce9afc0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = nn.evaluate(vectorizer.transform(X_test).toarray(), y_test, verbose = 1)\n",
    "# scores has several measurements in it. The one in postion 1 is the accuracy.\n",
    "print(\"Accuracy:\", scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08a698b-4450-4b96-bab0-494898974400",
   "metadata": {},
   "source": [
    "We try out the model with our own comment. We need to pre-process the new comment like we did the training comments. This is best put in a function or pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07acb6-edbe-4d1c-8fee-4ce8750aacf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(comment):\n",
    "    comment.replace(\"NEWLINE_TOKEN\", \"\")\n",
    "    comment.replace(\"TAB_TOKEN\", \"\")\n",
    "    comment.lower()\n",
    "    comment = re.sub(r'[^\\w\\s]', '', comment)\n",
    "    comment = re.sub(r'\\d+', '', comment)\n",
    "    return(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e181567-8c0a-4157-ac2b-61bfe6999d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_analysis(raw_comment):\n",
    "    prepared_comment = pd.array([prepareData(raw_comment)])\n",
    "    vectorized_comment = vectorizer.transform(prepared_comment)\n",
    "    print(\"Input: \", raw_comment)\n",
    "    print(\"Probability that it is a personal attack :\", nn.predict(vectorized_comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea31baf-044b-4eb3-a9b0-fc011a5c74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_analysis(\"This is a terrible article. Whoever wrote it is a total fool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d541d21e-8bec-4a12-a409-2db20712b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_analysis(\"This is the best article on this topic. Thank you for writing it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c940a-28dc-4e9f-9fe1-35dd3b5af93d",
   "metadata": {},
   "source": [
    "*Try some comments of your own*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a5497-a3e2-44e2-88c5-f66e2a8a00a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.perf_counter()\n",
    "print(\"Time taken: in min\", (end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3539d5e9-dd3b-45e8-a928-2f9b085a1efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
