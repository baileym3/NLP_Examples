{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 2.2 Classification with a RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the IMDB dataset which has 50K movie reviews. This is a dataset for binary sentiment classification. The reviews are labelled positive (1) and negative (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore this -- it is just for timing how long the program runs\n",
    "import time\n",
    "start = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_JASVhPkkgb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "from keras.layers import Embedding\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data willll be obtained with words represented by integer encodings based on word frequency. We shall use the 5000 most frequently used words as the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape is \", X_train.shape)\n",
    "print(\"y_train shape is \", y_train.shape)\n",
    "print(\"X_test shape is \", X_test.shape)\n",
    "print(\"y_test shape is \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 25000 training records and 25000 testing records. Because time is short we shall only use 2000 of each. The model will not perform well but you will see the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[0:2000]\n",
    "y_train = y_train[0:2000]\n",
    "X_test = X_test[0:2000]\n",
    "y_test = y_test[0:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape is \", X_train.shape)\n",
    "print(\"y_train shape is \", y_train.shape)\n",
    "print(\"X_test shape is \", X_test.shape)\n",
    "print(\"y_test shape is \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at a record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features of first record\", X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integers are word IDs pre-assigned to words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the label of the first record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label of first record\",y_train[0])\n",
    "# Label is 1 (positive) or 0 (negative sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The record has  label 1(positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the word index and create a reverse index so that we can see the words. Since only the most fewqquent 5000 words are used there will be a lot missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = imdb.get_word_index()\n",
    "id2word = {i:word for word, i in word2id.items()}\n",
    "print([id2word.get(i, ' ') for i in X_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we feed the reviews to the model, we will either trim or pad them so that they are all the same length. We therefore should take a look at some statistics on the lengths of theee records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset =np.hstack((X_train,X_test))\n",
    "total_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(total_dataset)\n",
    "lengths = df[0].apply(len)\n",
    "print(\"max length of record: \",lengths.max())\n",
    "print(\"75% of the records are of length less than : \",lengths.quantile(0.75))\n",
    "print(\"mean length of record: \",lengths.mean())\n",
    "print(\"25% of the records are of length less than : \",lengths.quantile(0.25))\n",
    "print(\"min length of record: \",lengths.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pad the records so that they are all of length 500.\n",
    "Longer records will be truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "max_words = 500\n",
    "X_train = pad_sequences(X_train, maxlen = max_words)\n",
    "X_test = pad_sequences(X_test, maxlen = max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The records will be converted into vectors each of length 32. Kerass has a word embedding component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It saves on training time if you give the data to the model in batches rather than one record at a time. We say the RNN is stateful if during training the ending hidden state of a batch is the initial hidden state of the next batch while stateless means the initial hidden state for each batch is random. This makes a difference if the order of the records matter so that one batch follows on from the previous one. In this case it does not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_xxS29ykyVa"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Embedding.\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length = max_words, batch_input_shape=(batch_size, max_words)) )\n",
    "# Recurrent layer. Each  of the 2000 records is now a vector of size 32.\n",
    "model.add(SimpleRNN(64, input_shape = (2000,32), return_sequences = False,stateful = False))\n",
    "# Fully connected layer.\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "# Output layer.\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wekYoQp_kzGW"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Takes about 2 min.\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0, batch_size = batch_size)\n",
    "print(\"Accuracy\", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.perf_counter()\n",
    "print(\"Time taken: in min\", (end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Exercise_24_Stateless to Stateful.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
