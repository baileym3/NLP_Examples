{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models from OpenAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpq5pbtiZxBN"
   },
   "source": [
    "This exercise uses openAPI's rest endpoints. You will need an API key from openapi.com and there is a small charge for chatGPT3, chatGPT4 type models. If you get an API key then you will paste it into the file key.txt and delete the file when you are done. Otherwise just read the file to see the results that were obtained earlier with a key installed.  \n",
    "You will notice that the models are not downloaded to your VM. This is because, unlike GPT2 type models, the GPT3 and GPT4 are too large to run on a regular laptop. Instead, there  is a REST interface and you get charged by the number of tokens input as a prompt and the number geneated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API reference is at \n",
    "https://platform.openai.com/docs/api-reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14276,
     "status": "ok",
     "timestamp": 1681826504240,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": 240
    },
    "id": "Kdu6imofaR0u",
    "outputId": "52346e6d-9f33-4669-fee0-2d54c554cf2c"
   },
   "outputs": [],
   "source": [
    "#No need to run this cell. It only needs to be done once and it was done already\n",
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1414,
     "status": "ok",
     "timestamp": 1681827535062,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": 240
    },
    "id": "tQx9rzUbZyOV"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1681827537226,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": 240
    },
    "id": "-fqctzJCceHi"
   },
   "outputs": [],
   "source": [
    "openai.api_key = open(\"key.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easy to generate prompts, we will write a method, generate_prompt. In this example we want a prompt to ask the model for good names for an animal. Notice that to get better results we give the model some examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1681827539619,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": 240
    },
    "id": "MbU_fVP5bd9c"
   },
   "outputs": [],
   "source": [
    "def generate_prompt(animal):\n",
    "    return \"\"\"Suggest three names for an animal that is a superhero.\n",
    "\n",
    "Animal: Cat\n",
    "Names: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\n",
    "Animal: Dog\n",
    "Names: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\n",
    "Animal: {}\n",
    "Names:\"\"\".format(\n",
    "        animal.capitalize()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the method call we include the model we want to use. The temperature is a number between 0 and 1. When set to 0, the model always chooses the most probable next word. Running the method call again will give the same result. By making the temperature >0 you allow the model to sometimes pick a word that was not the one of highest probability. Choosing too high a temperature makes the result vary more but you tend to get more results that don't make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1107,
     "status": "ok",
     "timestamp": 1681827544250,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": 240
    },
    "id": "WR3yWSXhZz78",
    "outputId": "8e753c34-7247-42e8-a4ee-5281f5179292"
   },
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=generate_prompt(\"Rat\"),\n",
    "            temperature=0.6,\n",
    "        )\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only interested in the generated text so let's print it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 956,
     "status": "ok",
     "timestamp": 1681827549065,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": 240
    },
    "id": "032134HFcIga",
    "outputId": "9240812a-5f18-4564-f6d7-0fe73ac681d4"
   },
   "outputs": [],
   "source": [
    "response[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 909,
     "status": "ok",
     "timestamp": 1681827558126,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": 240
    },
    "id": "GwjOx5pWdQ-R",
    "outputId": "17817e99-1f1a-402b-8b6c-2abdfed2522c"
   },
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=\"create a tagline for a pizza shop\",\n",
    "            temperature=0.6,\n",
    "        )\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 981,
     "status": "ok",
     "timestamp": 1681827562124,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": 240
    },
    "id": "tJFaXXz3eWN5",
    "outputId": "e31ad2b2-8382-4026-97ed-50d17dbbae22"
   },
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=\"Tell me about inferential statistics\",\n",
    "            temperature=0.6,\n",
    "        )\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make it a little longer by making the max_tokens attribute bigger than the default 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=\"Tell me about inferential statistics\",\n",
    "            temperature=0.6,\n",
    "            max_tokens=100\n",
    "        )\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1681827570038,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": 240
    },
    "id": "RuhAK5CYsB7h",
    "outputId": "416dd860-eb0b-4642-fc30-e593f720ce29"
   },
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=\"\"\"Translate this into Spanish: \n",
    "             The nightingale is singing\"\"\",\n",
    "            temperature=0.6,\n",
    "        )\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1681827571790,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": 240
    },
    "id": "bBEfzEmuspLi"
   },
   "outputs": [],
   "source": [
    "textToSummarize = '''Amends the McKinney-Vento Homeless Assistance Act to redefine\n",
    "\"homeless\", \"homeless individual\", or \"homeless person\" to include:\n",
    "(1) a child or youth who has been verified as homeless by a local educational agency (LEA) homeless liaison, and the family of such child or youth; \n",
    "(2) a youth verified as homeless by the director (or a designee) of a program funded under the Runaway and Homeless Youth Act; \n",
    "(3) a child verified as homeless by the program director (or designee) under the Individuals with Disabilities Education Act, and the child's family; and \n",
    "(4) a child verified as homeless by the Head Start program director (or designee) under the Head Start Act, and the child's family. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3281,
     "status": "ok",
     "timestamp": 1681827579079,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": 240
    },
    "id": "P5-JDh-XuF_L",
    "outputId": "e09cefc4-3711-4216-c1ed-c78750bff192"
   },
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=\"Summarize this for a fourth grade student: \"+ textToSummarize ,\n",
    "            temperature=0.6,\n",
    "            max_tokens = 50\n",
    "        )\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LiDLLrd04sZ"
   },
   "source": [
    "## Chat completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You provide the conversation up to this point.\n",
    "The first message is usually for the role \"system\" to give the model some idea about what its role in the conversation is. \n",
    "You then give the conversation between the roles \"user\" and \"assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2494,
     "status": "ok",
     "timestamp": 1681827583901,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": 240
    },
    "id": "ACz4RptZ06Yv",
    "outputId": "da83760f-ab83-42f4-c064-219f4a530bea"
   },
   "outputs": [],
   "source": [
    "openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4337,
     "status": "ok",
     "timestamp": 1681827589591,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": 240
    },
    "id": "wVLkAtBDuTzo"
   },
   "outputs": [],
   "source": [
    "result = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a teacher.\"},\n",
    "        {\"role\": \"user\", \"content\": \"I would like to take a make-up exam because I was not here yesterday\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Why were you not here yesterday?\"},\n",
    "        {\"role\": \"user\", \"content\": \"I had a fender bender\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 605,
     "status": "ok",
     "timestamp": 1681827591371,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": 240
    },
    "id": "2ew4do4f2OhS",
    "outputId": "f09e449f-7458-4655-be22-310bcb89a688"
   },
   "outputs": [],
   "source": [
    "print(result[\"choices\"][0][\"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PczU_G023Uyo"
   },
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxHmiydB3Xdh"
   },
   "outputs": [],
   "source": [
    "response = openai.Image.create(\n",
    "  prompt=\"Jay Leno riding a bicycle\",\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "image_url = response['data'][0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1680572795855,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": 240
    },
    "id": "nmp4YTR33ZRV",
    "outputId": "30a1d928-2722-4aa2-de21-7e143fe22e2f"
   },
   "outputs": [],
   "source": [
    "image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOImYdJA7i9Jyjd2OCvVteP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
