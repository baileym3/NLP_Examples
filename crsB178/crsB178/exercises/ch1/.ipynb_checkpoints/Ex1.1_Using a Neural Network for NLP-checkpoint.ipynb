{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19900,
     "status": "ok",
     "timestamp": 1693822922187,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": -60
    },
    "id": "7SskWoniwU5K",
    "outputId": "af0749b4-0dc9-497c-95c4-4294e3040aed"
   },
   "source": [
    "# Ex1.1 Using a Neural Network for Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "error",
     "timestamp": 1693822926491,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": -60
    },
    "id": "f31svIolfAjZ",
    "outputId": "25acad63-90f3-437c-ed78-9d4818614cca"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall use just 100 comments from Wikipedia. A real model would need a lot more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_comment_small_100.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We examine the first few comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall define a function for cleaning the comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1693822835253,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": -60
    },
    "id": "SM9JEmpjfAji"
   },
   "outputs": [],
   "source": [
    "def clean_comment(text):\n",
    "    # Strip HTML tags\n",
    "    text = re.sub('<[^<]+?>', ' ', text)\n",
    "\n",
    "    # Strip escaped quotes\n",
    "    text = text.replace('\\\\\"', '')\n",
    "\n",
    "    # Strip quotes\n",
    "    text = text.replace('\"', '')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1693822835253,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": -60
    },
    "id": "f5YLNxvFwwJ_"
   },
   "outputs": [],
   "source": [
    "df['cleaned_comment'] = df['comment_text'].apply(clean_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall split the data into a test set and a training set. 20% of the data will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1693822835254,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": -60
    },
    "id": "KRe_aq0mfAjo"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_comment'], \n",
    "                                                    df['toxic'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk library will remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1693822835254,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": -60
    },
    "id": "An5Har1E9wV2"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to vectorize the documents. We shall use a word frequency approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1693822835254,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": -60
    },
    "id": "NK0BUVW7fAjt"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'),\n",
    "                             lowercase=True, min_df=3, max_df=0.9, max_features=5000)\n",
    "X_train_onehot = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1693822835255,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": -60
    },
    "id": "rQZCZuzafAjz"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "nn = Sequential()\n",
    "\n",
    "nn.add(Dense(units=500, activation='relu', input_dim=len(vectorizer.get_feature_names())))\n",
    "nn.add(Dense(units=500, activation='relu', input_dim=500))\n",
    "nn.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary method gives a summary of the architecture of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a numpy array rather than a sparse array for input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_onehot= X_train_onehot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train using the last 20 records of the training set for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1693822835255,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": -60
    },
    "id": "UQXxBfVSfAj4"
   },
   "outputs": [],
   "source": [
    "nn.fit(X_train_onehot[:-20], y_train[:-20],\n",
    "          epochs=5, batch_size=128, verbose=1,\n",
    "          validation_data=(X_train_onehot[-20:], y_train[-20:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the accuracy of our model against the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1693822835256,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": -60
    },
    "id": "_GnSscMifAj9"
   },
   "outputs": [],
   "source": [
    "scores = nn.evaluate(vectorizer.transform(X_test).toarray(), y_test, verbose=1)\n",
    "print(\"Accuracy:\", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1693822835256,
     "user": {
      "displayName": "Chris Mawata",
      "userId": "00563691954886767594"
     },
     "user_tz": -60
    },
    "id": "luM_AIsfkRUf"
   },
   "outputs": [],
   "source": [
    "# If we wanted to we could save the model and load it later\n",
    "nn.save('nn.hd5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us ask the model to predict three sample strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = \" I don't know if those TOC links are entirely useful. Wikipedia needs to come up with a way to link to items within a large table like that. Perhaps we could use the HTML code with links with the # like that?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = \"You will not stop a JEDI KNIGHT, stop trying to rule galaxy. And know the power of the darkside!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample3 = \"\"\"And I suggest you are failing if the result of your activity is to lower the boom on these editors as opposed to Mr. Ainsworth and his cohort. You are helping in the collection of scalps.,0\n",
    "\"Hey you, can you please tell me how my edit on the University of China page was considered a personal attack, when it was actually factual speaking?\n",
    " 03alpe01 is my name and we have the University of China, right here in Southampton,Poland! \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sample inputs have to be prepared the same way as the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vectorizer.transform([sample1, sample2, sample3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
